{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC-zlnaFaSCa"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "<font face=\"XB Zar\" size=5>\n",
        "<div align=center>\n",
        "<font face=\"B Titr\" size=5>\n",
        "<p></p><p></p>\n",
        "بسمه تعالی\n",
        "<p></p>\n",
        "</font>\n",
        "<p></p>\n",
        "<font>\n",
        "<br>\n",
        "درس بازیابی پیشرفته اطلاعات\n",
        "<br>\n",
        "مدرس: دکتر بیگی\n",
        "</font>\n",
        "<p></p>\n",
        "<br>\n",
        "<font>\n",
        "<b>تمرین اول</b>\n",
        "</font>\n",
        "<br>\n",
        "<br>\n",
        "موعد تحویل: ۱۵ آبان <br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<font>\n",
        "دانشگاه صنعتی شریف\n",
        "<br>\n",
        "دانشکده مهندسی کامپیوتر\n",
        "<br>\n",
        "<br>\n",
        "</font>\n",
        "</div>\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TijGLPpaSCb"
      },
      "source": [
        "مبینا سلیمی پناه - ۹۹۱۰۹۷۸۸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-P08VIxaSCb"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=6>\n",
        "<h1>مقدمه</h1>\n",
        "</font>\n",
        "<font face=\"XB Zar\" size=3>\n",
        "این تمرین به پیش‌پردازش متن، اصلاح پرسمان، ساخت نمایه، بازیابی boolean و فشرده‌سازی نمایه می‌پردازد.\n",
        "<br>\n",
        "دیتاستی که در اختبار شما قرار گرفته است شامل چکیده مقالات و id آن‌ها می‌باشد.\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ4JaC2uaSCc"
      },
      "source": [
        "<div dir='rtl'>\n",
        "<h1> آماده‌سازی دیتاست</h1>\n",
        "<p>\n",
        "دیتاستی که در اختیار شما قرار گرفته است، دارای سطر‌هایی می‌باشد که دارای مقدار NaN می‌باشد. برای اینکه بتوانید با این دیتاست کار کنید، باید ابتدا این سطر‌ها را حذف کنید.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpQsIi9-aSCc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('data.csv')\n",
        "new_data = data.dropna()\n",
        "dcm = [(i, j) for i, j in zip(new_data['paperId'], new_data['abstract'])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV8GCWXwaSCc"
      },
      "source": [
        "<div dir='rtl'>\n",
        "<h1> Preprocessing (پیش پردازش)</h1>\n",
        "<p>\n",
        "بسیاری از داده ‌ها دارای مقادیر زیادی اطلاعات اضافه هستند که در پردازش ها به آن نیازی نیست و یا باعث ایجاد خطا میشوند.\n",
        "دراین بخش داده را از دیتابیس مورد نظر خوانده\n",
        "  و سپس پیش پردازش های مورد نیاز را اعمال کنید تا متن پیش پردازش شده را تولید کنید.\n",
        "پس از اتمام پیش پردازش سایر عملیات گفته شده در ادامه را بر\n",
        "روی متن ایجاد شده انجام میدهیم.\n",
        "\n",
        "\n",
        "کلاس\n",
        "\"Preprocessor\"\n",
        "عملیات پیش پردازش را انجام میدهد. نام توابع عمل های مورد نظر نوشته شده است و که با توجه به آن باید کد مخصوص هر یک نوشته شود. تابع\n",
        "\"preprocessor\"\n",
        "تابع اصلی این کلاس است که متن بدون پیش پردازش را گرفته و پردازش های مورد نظر را در آن اعمال میکند و متن مورد نظر را ایجاد میکند.\n",
        "\n",
        "در این بخش میتوانید از کتابخانه های آماده مانند\n",
        "<a href=\"https://www.nltk.org/\">NLTK</a>\n",
        "و\n",
        "<a href=\"https://spacy.io/\">SpaCy</a>\n",
        "استفاده کنید.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky6SFiMxaSCc"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppCwpoHxaSCc",
        "outputId": "5b4ab22d-d56a-4ae6-9759-4688b6972c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_H_f-0RaSCd"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Preprocessor:\n",
        "\n",
        "    def __init__(self, stopwords_path = None):\n",
        "      # Create a variable of stop words.\n",
        "\n",
        "        if stopwords_path:\n",
        "            with open(stopwords_path, 'r') as file:\n",
        "                for f in file:\n",
        "                    line = \"\"\n",
        "                    for char in f:\n",
        "                        if char.strip():\n",
        "                            line += char\n",
        "                    self.stopwords.append(line)\n",
        "        else:\n",
        "            self.stopwords = set(stopwords.words('english'))\n",
        "\n",
        "    def preprocess(self, text):\n",
        "      # The main function of the class.\n",
        "        text = self.normalize(text)\n",
        "        text = self.remove_links(text)\n",
        "        text = self.remove_punctuations(text)\n",
        "        words = self.word_tokenize(text)\n",
        "        words = self.remove_stopwords(words)\n",
        "        return words\n",
        "\n",
        "    def normalize(self, text):\n",
        "        # Normalize text (lower case, stemming, lemmatization, etc.)\n",
        "        text = text.lower()\n",
        "        words = self.word_tokenize(text)\n",
        "\n",
        "        #stemming\n",
        "        stemmed_words = []\n",
        "        for w in words:\n",
        "            stemmed_words.append(PorterStemmer().stem(w))\n",
        "        words = stemmed_words\n",
        "\n",
        "        #lemmatizing\n",
        "        words = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
        "\n",
        "        return ' '.join(words)\n",
        "\n",
        "\n",
        "    def remove_links(self, text):\n",
        "        # Remove links\n",
        "        return re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    def remove_punctuations(self, text):\n",
        "        # Remove punctutaions\n",
        "        return re.sub(r'[^\\w\\s]','',text)\n",
        "\n",
        "    def word_tokenize(self, text):\n",
        "        # Tokenize text\n",
        "        return nltk.word_tokenize(text)\n",
        "\n",
        "    def remove_stopwords(self, words):\n",
        "        # Remove stopwords\n",
        "        not_stopwords = []\n",
        "        for word in words:\n",
        "            if word not in self.stopwords:\n",
        "                not_stopwords.append(word)\n",
        "        return not_stopwords\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4Md59bMaSCd"
      },
      "source": [
        "<div dir='rtl'>\n",
        "<h1>ساخت نمایه</h1>\n",
        "<p>\n",
        "شما در حال توسعه یک موتور جستجوی سریع هستید که از نمایه سازی پویا پشتیبانی می کند. موتور جستجو اسناد جدید را در قالب دسته‌هایی کوچک‌تر   \n",
        "(batch) هندل می‌کند. در پایان هر روز، این دسته‌ها با استفاده از استراتژی ادغام لگاریتمی ادغام می شوند. هدف به حداقل رساندن هزینه ادغام است.  \n",
        "مراحلی که باید برای حل این مسئله انجام دهید عبارتند از:\n",
        "<li>توکن‌بندی و نرمال‌سازی متن از اسناد.</li>\n",
        "    <li>ایجاد یک index مرتب‌ شده برای هر دسته از اسناد.</li>\n",
        "    <li>ادغام بهینه چند دسته از indexها با استفاده از یک استراتژی ادغام لگاریتمی.</li>\n",
        "وظیفه شما این است که بخش‌های خالی <strong>(مشخص شده به‌صورت {TODO})</strong> کد را پر کنید تا موتور جستجو عملی شود.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRwj9pPiaSCd"
      },
      "source": [
        "<div dir='rtl'>\n",
        "<h3> دستورات </h3>\n",
        "<li>متد <code>sort_based_index_construction</code> از <code>DocumentBatch</code>:</li>\n",
        "<p>هر سند را با استفاده از تابع‌هایی که در قسمت قبل نوشتید، عمل preprocessing را روی آن انجام دهید.</p>\n",
        "<p>برای هر توکن، شناسه سند را به فهرست معکوس (inverted index) برای آن توکن اضافه کنید.</p>\n",
        "<li>متد <code>add_batch</code> در <code>FastSearchEngine</code></li>\n",
        "<p>فهرست معکوس برای دسته (batch) را ایجاد کنید.</p>\n",
        "<p>این دسته را به فهرست‌های روزانه اضافه کنید.</p>\n",
        "<li>متد <code>end_of_day_merge</code> از <code>FastSearchEngine:</code></li>\n",
        "<p>استراتژی ادغام لگاریتمی را پیاده‌سازی کنید تا به صورت بهینه فهرست‌های روزانه را با فهرست اصلی ادغام کنید.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUOyuXV2aSCd"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, deque\n",
        "\n",
        "class DocumentBatch:\n",
        "    def __init__(self, docs):\n",
        "        self.documents = docs\n",
        "        self.index = defaultdict(set)\n",
        "        self.preprocessor = Preprocessor()\n",
        "\n",
        "    def sort_based_index_construction(self):\n",
        "        #print(self.documents)\n",
        "\n",
        "        # {TODO}: Build an inverted index for the batch\n",
        "        for i , text in self.documents:\n",
        "            tokens = self.preprocessor.preprocess(text)\n",
        "            for token in tokens:\n",
        "                self.index[token].add(i)\n",
        "        pass\n",
        "\n",
        "class FastSearchEngine:\n",
        "    def __init__(self):\n",
        "        self.main_index = defaultdict(set)\n",
        "        self.daily_indices = deque()\n",
        "\n",
        "    def add_batch(self, batch: DocumentBatch):\n",
        "        # {TODO}: Incorporate the new batch into the daily indices\n",
        "        batch.sort_based_index_construction()\n",
        "        self.daily_indices.append(batch.index)\n",
        "        pass\n",
        "\n",
        "    def end_of_day_logarithmic_merge(self):\n",
        "        # Merge the daily indices using the logarithmic merge algorithm\n",
        "\n",
        "        while len(self.daily_indices) > 1:\n",
        "            index_list = defaultdict(set)\n",
        "\n",
        "            for word, doc_ids in self.daily_indices.popleft().items():\n",
        "                index_list[word].update(doc_ids)\n",
        "\n",
        "            for word, doc_ids in self.daily_indices.popleft().items():\n",
        "                index_list[word].update(doc_ids)\n",
        "\n",
        "            self.daily_indices.append(index_list)\n",
        "\n",
        "        if self.daily_indices:\n",
        "            for word, doc_ids in self.daily_indices.popleft().items():\n",
        "                self.main_index[word].update(doc_ids)\n",
        "\n",
        "        pass\n",
        "\n",
        "\n",
        "# Divide the documents into groups to distribute them between servers. For example, let's consider two servers here.\n",
        "# Divide the documents of each server into batches, for instance, five batches.\n",
        "# Create an index for each batch and for each server, then merge them into the main index at the end of the day.\n",
        "# Repeat this process until all documents are processed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkCrhAqcaSCe",
        "outputId": "0e7443e5-74c3-4e2b-bbdb-05ba3c4278f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'set'>, {'sample_9': {8}, 'sample_10': {9}, 'sample_19': {8}, 'sample_20': {9}})\n"
          ]
        }
      ],
      "source": [
        "#Usage example\n",
        "server_a_docs = [\"sample_1\", \"sample_2\", \"sample_3\", \"sample_4\", \"sample_5\", \"sample_6\", \"sample_7\", \"sample_8\", \"sample_9\", \"sample_10\"]\n",
        "server_b_docs = [\"sample_11\", \"sample_12\", \"sample_13\", \"sample_14\", \"sample_15\", \"sample_16\", \"sample_17\", \"sample_18\", \"sample_19\", \"sample_20\"]\n",
        "server_a_docs = [(index, string) for index, string in enumerate(server_a_docs)]\n",
        "server_b_docs = [(index, string) for index, string in enumerate(server_b_docs)]\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    server_a_batch = DocumentBatch(server_a_docs[i*2:i*2+2])\n",
        "    server_b_batch = DocumentBatch(server_b_docs[i*2:i*2+2])\n",
        "\n",
        "    search_engine = FastSearchEngine()\n",
        "    search_engine.add_batch(server_a_batch)\n",
        "    search_engine.add_batch(server_b_batch)\n",
        "\n",
        "    # At end of day\n",
        "    search_engine.end_of_day_logarithmic_merge()\n",
        "\n",
        "# Note that the above was just an example demonstrating the general process.\n",
        "# but you can take two servers and five batches for each server as a constant and implement the process for them.\n",
        "print(search_engine.main_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6BQHMo3aSCe"
      },
      "source": [
        "<font color='red'>\n",
        "برای بررسی این قسمت میتوانیم با استفاده از تابع <ایندکسینگ داک> که در قسمت آخر تمرین نوشته شده است کل دیتا را نیز ایندکس کرده و سپس با هم مرج کنیم.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HXFzZOgaSCe"
      },
      "source": [
        "<div dir='rtl'>\n",
        "<h1>Spell Correction (اصلاح پرسمان)</h1>\n",
        "<p>\n",
        "در بسیاری از اوقات پرسمان دادە شده توسط کاربر، ممکن است ناقص یا دارای غلط املایی باشد. برای رفع این مشکل در بسیاری از موتورهای جستجو راە حل هایی تدارک دیده شدە است. ابتدا این راە حل ها را شرح دهید و بیان کنید که یک موتور جست و جو بر چه اساسی پرسمان های اصلاح شده را به کاربر نمایش می دهد.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ujpAf4_aSCe"
      },
      "source": [
        "<div dir='rtl'>\n",
        "<h2>پاسخ سوال بالا</h2>\n",
        "پاسخ‌ خود را در این سلول بنویسید."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hacBhOmaSCe"
      },
      "source": [
        "برخی راه حل‌های ارائه شده توسط موتورهای جستجو:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0ATscwVaSCe"
      },
      "source": [
        "موتورهای جستجو از الگوریتم‌های تشخیص غلط املایی استفاده می‌کنند تا کلماتی که احتمالاً به اشتباه تایپ شده‌اند را تشخیص دهند.\n",
        "سپس با استفاده از دیکشنری‌های املایی و محاسبه فاصله ویرایش، پیشنهادات تصحیح را ارائه می‌دهند"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4n81pDUaSCe"
      },
      "source": [
        "\n",
        "هنگامی که کاربر در حال تایپ یک کلمه یا عبارت است، موتور جستجو به صورت زنده پیشنهادهایی را بر اساس جستجوهای پرتکرار و محبوب ارائه می‌دهد.\n",
        "این کار کمک می‌کند که کاربران سریع‌تر به پرسمان مورد نظر خود برسند و از غلط املایی جلوگیری شود."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_XQivgoaSCe"
      },
      "source": [
        "برخی موتورهای جستجو از مدل‌های یادگیری ماشینی و پردازش زبان طبیعی برای درک بهتر نیت کاربران از پرسمان‌ها استفاده می‌کنند.\n",
        "این رویکرد به موتور جستجو کمک می‌کند که حتی در صورت نادرست بودن یا ناقص بودن پرسمان، منظور کاربر را بهتر درک کرده و نتایج مرتبط‌تری را نمایش دهد.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_JnNkBjaSCe"
      },
      "source": [
        "موتور جستجو بر اساس الگوریتم‌های داخلی خود و با توجه به داده‌هایی مانند تاریخچه جستجوی کاربر، میزان محبوبیت پرسمان‌ها، و میزان ارتباط پرسمان با نتایج جستجو، پرسمان‌های اصلاح شده را به کاربر نمایش می‌دهد."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP0WFHx6aSCe"
      },
      "source": [
        "----------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKCE2mKWaSCe"
      },
      "source": [
        "<div dir='rtl'>\n",
        "<p>\n",
        "    در این بخش، ابتدا با استفاده از روش bigram لغات نزدیک به لغات اصلی را پیدا کنید و در آخر با معیار minimum edit distance لغتی جایگزین را برای لغت مورد نظر پیدا کنید.  سپس برای هر پرسمان ورودی کاربر، در صورت اشتباه بودن آن، آن را تصحیح کنید. برای stopword‌ها نیز می‌توانید از لیست موجود که در قسمت‌های قبل ساختید استفاده کنید.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8X-TCRuaSCe"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "\n",
        "def create_bigram_index(words):\n",
        "    \"\"\"\n",
        "    Creates a bigram index for the spell correction\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        A dictionary of bigrams and their occurence\n",
        "    \"\"\"\n",
        "    # TODO: Create the bigram index here\n",
        "\n",
        "    bigram_index: Dict[str, List[str]] = defaultdict(set)\n",
        "    for w in words:\n",
        "        bigram_index['$' + w[0]].add(w)\n",
        "        for i in range(len(w) - 1):\n",
        "            bigram = w[i:i+2]\n",
        "            bigram_index[bigram].add(w)\n",
        "    return bigram_index\n",
        "\n",
        "\n",
        "\n",
        "#print(bigram_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6DnD3laaSCe",
        "outputId": "806fb767-a883-469f-d807-9cb0c7814e47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the most popular programming language ?\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "def levenshtein_distance(s1, s2):\n",
        "\n",
        "    edit_matrix = [[0] * (len(s2) + 1) for k in range(len(s1) + 1)]\n",
        "    for i in range(len(s1) + 1):\n",
        "        for j in range(len(s2) + 1):\n",
        "            edit_matrix[i][j] = 0\n",
        "\n",
        "    for i in range(len(s1) + 1):\n",
        "        for j in range(len(s2) + 1):\n",
        "            if i == 0:\n",
        "                edit_matrix[i][j] = j\n",
        "\n",
        "            elif j == 0:\n",
        "                edit_matrix[i][j] = i\n",
        "\n",
        "            elif s1[i - 1] == s2[j - 1]:\n",
        "                edit_matrix[i][j] = edit_matrix[i - 1][j - 1]\n",
        "\n",
        "            else:\n",
        "                edit_matrix[i][j] = min(edit_matrix[i][j - 1] + 1,  # Insert\n",
        "                               edit_matrix[i - 1][j] + 1,  # Remove\n",
        "                               edit_matrix[i - 1][j - 1] + 2)  # replace = remove + insert ?!\n",
        "\n",
        "    return edit_matrix[len(s1)][len(s2)]\n",
        "\n",
        "\n",
        "\n",
        "def spell_correction(query,bigram_index):\n",
        "\n",
        "    corrected_query_tokens = []\n",
        "    tokens = nltk.word_tokenize(query)\n",
        "\n",
        "    for token in tokens:\n",
        "        if token in string.punctuation:\n",
        "            corrected_query_tokens.append(token)\n",
        "            continue\n",
        "\n",
        "        elif token in Preprocessor().stopwords:\n",
        "            corrected_query_tokens.append(token)\n",
        "            continue\n",
        "\n",
        "        token_bigrams = ['$' + token[0]]\n",
        "        for i in range(0, len(token)-1):\n",
        "            token_bigrams.append(token[i:i+2])\n",
        "\n",
        "        possible_terms = []\n",
        "\n",
        "        for bigram in token_bigrams:\n",
        "            possible_terms.extend(bigram_index.get(bigram, []))\n",
        "\n",
        "        possible_terms = Counter(possible_terms)\n",
        "        sorted_terms = sorted(possible_terms.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        sorted_dict = {term: count for term, count in sorted_terms}\n",
        "\n",
        "        closest_term = min(sorted_dict, key=lambda term: levenshtein_distance(term, token), default=token)\n",
        "        corrected_query_tokens.append(closest_term)\n",
        "\n",
        "    corrected_query = ' '.join(corrected_query_tokens)\n",
        "    return corrected_query\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "words = set()\n",
        "for _, doc in dcm:\n",
        "    tokens = nltk.word_tokenize(doc)\n",
        "    if len(tokens) > 0:\n",
        "        words.update(tokens)\n",
        "\n",
        "\n",
        "\n",
        "bigram_index = create_bigram_index(words)\n",
        "\n",
        "\n",
        "#Example usage\n",
        "user_query = \"Wht is the most populr progarmming lanuage?\"\n",
        "print(spell_correction(user_query, bigram_index))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYoOGY5PaSCf"
      },
      "source": [
        "<div dir='rtl'>\n",
        "<h1> Boolean Retrieval </h1>\n",
        "<p>\n",
        " در این قسمت هدف طراحی یک سامانەی بازیابی اطلاعات boolean می‌باشد.\n",
        "\n",
        "برای این کار ابتدا پیش پردازش‌های مورد نیاز را مانند بخش قبل بر روی متون انجام دهید و در مرحله بعد ماتریس doc−term را ایجاد کنید. در نهایت کلاس BooleanRetrievalModel را تکمیل کنید که شامل توابع preprocess_query و find_siⅿiⅼar_docs است که توضیحات هرکدام در قسمت کد موجود است. هدف نهایی این است که هرگاه کوئری به تابع find_siⅿiⅼar_docs از کلاس BooleanRetrievalModel داده شود، شناسه k تا از داک‌هایی که شامل کوئری داده شده هستند برگردانده شوند.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6aBiu-RaSCf"
      },
      "outputs": [],
      "source": [
        "# preprocess and tokenize all documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bID-iHOaSCf"
      },
      "outputs": [],
      "source": [
        "def make_doc_term_matrix(documents):\n",
        "    \"\"\"\n",
        "    Create doc_term_matrix by using documents and unique words.\n",
        "    \"\"\"\n",
        "    matrix = dict()\n",
        "    terms = set()\n",
        "\n",
        "    for i, text in documents:\n",
        "        words = Preprocessor().preprocess(text)\n",
        "        terms.update(words)\n",
        "\n",
        "\n",
        "    for i, text in documents:\n",
        "        matrix[i] = {term: False for term in terms}\n",
        "        words = Preprocessor().preprocess(text)\n",
        "        for word in words:\n",
        "            if word in matrix[i]:\n",
        "                matrix[i][word] = True\n",
        "\n",
        "    return matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qbMlGIHaSCf"
      },
      "outputs": [],
      "source": [
        "class BooleanRetrievalModel:\n",
        "    def __init__(self, doc_term_matrix):\n",
        "        \"\"\"\n",
        "        Set doc_term_matrix and initialize the model.\n",
        "        \"\"\"\n",
        "        self.matrix = doc_term_matrix\n",
        "\n",
        "    def preprocess_query(self, query):\n",
        "        \"\"\"\n",
        "        Do necessary preprocessing here before using the query to find k similar docs.\n",
        "        Use methods from Preprocess section.\n",
        "        \"\"\"\n",
        "        processed_query = Preprocessor().preprocess(query)\n",
        "        return processed_query\n",
        "\n",
        "\n",
        "    def find_siⅿiⅼar_docs(self, query, k=20, and_query = False):\n",
        "        processed_query = self.preprocess_query(query)\n",
        "        similar_docs = []\n",
        "        count = 0\n",
        "\n",
        "        for doc_id, doc_terms in self.matrix.items():\n",
        "            doc_diffs = 0\n",
        "            threshold = 1\n",
        "\n",
        "            for token in processed_query:\n",
        "                if doc_diffs > threshold:\n",
        "                    break\n",
        "\n",
        "                if not doc_terms.get(token, False):\n",
        "                    doc_diffs += 1\n",
        "\n",
        "            if doc_diffs <= threshold:\n",
        "                similar_docs.append(doc_id)\n",
        "                count += 1\n",
        "\n",
        "            if count >= k:\n",
        "                break\n",
        "\n",
        "        return similar_docs[:k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KACC2tkJaSCf"
      },
      "source": [
        "<div dir='rtl'>\n",
        "در این قسمت ۳ کوئری مختلف به دلخواه خود بزنید و لیست داکیومنت‌های مرتبط با آن‌ها را برگردانید. برای کوتاه‌تر شدن لیست جواب در هر کوئری می‌توانید از عملگر‌های منطقی مانند AND استفاده کنید.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUi9miXNaSCf",
        "outputId": "61cdd90d-66bc-403d-f682-45922fa08bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1262, 1278, 1305, 1486, 1503, 1523, 1535, 1585, 1659, 1683, 1754, 1788, 1817, 1863, 1935, 1988, 2009, 2286, 2300, 2331]\n",
            "[2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026]\n",
            "[2009, 2015, 2260, 2286, 2300, 2331, 2465, 2584, 2631, 2682, 2797, 2814, 2816, 2887, 2914, 2947]\n"
          ]
        }
      ],
      "source": [
        "# make 3 queries and find similar docs for each of them\n",
        "\n",
        "query1 = \"python is a programming language\"\n",
        "print(BooleanRetrievalModel(make_doc_term_matrix(dcm[1000:3000])).find_similar_docs(query1))\n",
        "\n",
        "query2 = \"A Network \"\n",
        "print(BooleanRetrievalModel(make_doc_term_matrix(dcm[2000:3000])).find_similar_docs(query2))\n",
        "\n",
        "query3 = \"What is the most popular programming language\"\n",
        "print(BooleanRetrievalModel(make_doc_term_matrix(dcm[2000:3000])).find_similar_docs(query3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkegngjRaSCf"
      },
      "source": [
        "<div dir='rtl'>\n",
        "\n",
        "# ذخیره و فشرده‌سازی نمایه\n",
        "در این بخش، در ابتدا دو الگوریتم فشرده‌سازی gamma code و variable byte را پیاده‌سازی کنید.  \n",
        "سپس نمایه را به سه شکل زیر ذخیره کنید:\n",
        "- نمایه‌ی اصلی بدون فشرده‌سازی\n",
        "- نمایه‌ای که با استفاده از gamma code فشرده شده است.\n",
        "- نمایه‌ای که با استفاده از variable byte فشرده شده است.\n",
        "\n",
        "در ادامه اندازه‌ی هر کدام از سه فایل بالا را با استفاده از یک تابع به دست آورده و چاپ کنید.  \n",
        "همچنین باید تابع‌هایی برای decompress کردن نمایه‌های فشرده‌شده پیاده‌سازی کنید.\n",
        "\n",
        "**نکته‌ی ۱:** تمامی نمایه‌ها را نیز در کوئرا ارسال کنید. اگر حجم‌شان بیش‌تر از محدودیت کوئرا است، آن‌ها را در یک مکان دیگر آپلود کرده و لینک آن را در این فایل قرار دهید.  \n",
        "**نکته‌ی ۲:** توابع زیر صرفاً پیشنهادی هستند و هر گونه تغییر تا زمانی که کاربردهای مورد نظر پیاده شود، آزاد است.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRP---OnaSCf"
      },
      "outputs": [],
      "source": [
        "def gamma_encode(number):\n",
        "    if number == 0:\n",
        "        return '0'\n",
        "\n",
        "    binary_number = bin(number)[2:]\n",
        "    offset = '1' * (len(binary_number) - 1)\n",
        "    return offset + '1' + binary_number[1:]\n",
        "    pass\n",
        "\n",
        "def variable_byte_encode(number):\n",
        "    if number == 0:\n",
        "        return [0]\n",
        "\n",
        "    encoded_bytes = []\n",
        "    while number > 0:\n",
        "        encoded_bytes.append(number % 128)\n",
        "        number //= 128\n",
        "\n",
        "    encoded_bytes[-1] += 128\n",
        "    encoded_bytes.reverse()\n",
        "    new_encoded_bytes = []\n",
        "    for byte in encoded_bytes:\n",
        "        new_encoded_bytes.append(bytes([byte]))\n",
        "    encoded_bytes = new_encoded_bytes\n",
        "\n",
        "\n",
        "    return encoded_bytes\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muD0IxPmaSCf",
        "outputId": "7237d7ce-5a7c-41a6-cb46-322b0d11e228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'set'>, {'networkonchip': {104, 103}, 'noc': {103}, 'use': {103, 105, 106, 112, 113, 115, 118, 119, 122}, 'interest': {103}, 'option': {103}, 'design': {103, 104, 107, 108, 109, 110, 112, 113, 114, 117}, 'commun': {104, 109, 110, 103}, 'infrastructur': {104, 109, 110, 103}, 'embed': {113, 108, 109, 103}, 'system': {103, 106, 111, 114, 115, 116, 118, 119}, 'provid': {103}, 'scalabl': {119, 103}, 'structur': {114, 103}, 'balanc': {103}, 'core': {116, 106, 111, 103}, 'becaus': {103}, 'sever': {121, 114, 103}, 'data': {103, 107, 111, 112, 113, 115, 116, 119, 121}, 'packet': {103}, 'transmit': {103}, 'simultan': {105, 103}, 'network': {113, 121, 103, 105, 108}, 'effici': {103, 105, 106, 107, 111, 113, 117, 118}, 'rout': {103}, 'strategi': {117, 103}, 'must': {103}, 'order': {121, 119, 117, 103, 105}, 'avoid': {115, 111, 103}, 'congest': {103}, 'delay': {104, 103}, 'thi': {103, 104, 105, 106, 107, 108, 109, 110, 112, 114, 115, 116, 117, 118, 119, 121, 122}, 'paper': {103, 105, 106, 107, 108, 109, 110, 112, 114, 115, 116, 117, 121, 122}, 'ant': {103}, 'coloni': {103}, 'algorithm': {105, 122, 118, 103}, 'find': {112, 103, 120, 106, 110}, 'optim': {104, 113, 108, 103}, 'meshbas': {103}, 'driven': {103}, 'minim': {113, 122, 103}, 'total': {103}, 'latenc': {121, 106, 119, 103}, 'transmiss': {103}, 'simul': {105, 116, 118, 103}, 'result': {112, 119, 118, 103, 105, 106, 110}, 'show': {103, 104, 106, 108, 113, 115, 116, 117, 119}, 'effect': {112, 113, 114, 115, 103, 104, 121}, 'inspir': {103}, 'compar': {103, 107, 110, 111, 113, 114, 115, 119}, 'gener': {105, 109, 103}, 'purpos': {103}, 'deadlock': {103}, 'free': {103}, 'differ': {115, 118, 103}, 'traffic': {103}, 'pattern': {113, 103}, 'propos': {114, 115, 119, 104, 105, 108, 110}, 'new': {115, 119, 104, 121, 108}, 'accur': {104}, 'predict': {104, 111}, 'model': {104, 105, 108}, 'power': {112, 114, 122, 119, 104, 106, 111}, 'area': {104, 113}, 'buffer': {104, 115}, 'interconnect': {104}, 'enabl': {104, 114, 111}, 'systemlevel': {104}, 'explor': {104, 121, 116}, 'exist': {104, 122}, 'futur': {104, 113}, 'nanomet': {104}, 'technolog': {104, 115, 117}, 'process': {113, 116, 122, 104, 121, 106, 110}, 'significantli': {104, 121, 115}, 'previou': {104, 114}, 'essenti': {104, 117}, 'match': {104}, 'signoff': {104}, 'analys': {104, 120}, 'integr': {104}, 'cosiocc': {104}, 'synthesi': {104, 113}, 'impact': {104, 117}, 'feasibl': {104, 109}, 'architectur': {116, 104, 106, 108, 110}, 'synthes': {104}, 'tool': {104}, 'direct': {105}, 'repres': {105}, 'asymmetr': {105}, 'relationship': {105}, 'among': {105}, 'unit': {105}, 'coclust': {105}, 'aim': {105}, 'cluster': {105}, 'sender': {105}, 'receiv': {105}, 'particular': {105, 116}, 'wellknown': {105}, 'spectral': {105}, 'could': {105}, 'modifi': {105, 119}, 'howev': {114, 117, 105, 108, 110, 111}, 'largescal': {105}, 'pose': {105}, 'comput': {105, 107, 108, 110, 111, 113, 116, 117, 118}, 'challeng': {105, 113}, 'leverag': {105}, 'random': {105}, 'sketch': {105}, 'techniqu': {105, 114, 118}, 'acceler': {105, 121, 110, 113}, 'specif': {105, 121, 119}, 'deriv': {105}, 'two': {105, 106, 115, 116}, 'seri': {105}, 'one': {114, 115, 116, 117, 119, 105, 110}, 'randomprojectionbas': {105}, 'randomsamplingbas': {105}, 'theoret': {105}, 'analyz': {105, 106}, 'modelstextendash': {105}, 'emph': {105}, 'stochast': {105}, 'coblock': {105}, 'degre': {105, 118}, 'correct': {112, 105, 114, 119}, 'approxim': {105, 117}, 'error': {105, 114, 119}, 'rate': {105, 116, 119}, 'misclust': {105}, 'establish': {105}, 'indic': {105}, 'better': {105, 110}, 'bound': {105}, 'stateoftheart': {105, 115, 108}, 'literatur': {105}, 'numer': {105, 106, 113}, 'conduct': {105}, 'support': {105, 109, 113, 111}, 'test': {112, 105, 114}, 'real': {105}, 'ten': {105}, 'million': {105}, 'node': {105}, 'present': {112, 118, 106, 107, 109, 111}, 'detail': {120, 106, 117, 109}, 'firstofitskind': {106}, 'anatomi': {106}, 'commod': {106}, 'interact': {106}, 'wearabl': {106}, 'ask': {106}, 'question': {106}, '1': {106, 122}, 'deliv': {106}, 'closetomet': {106}, 'energi': {113, 114, 117, 122, 119, 106}, 'perform': {106, 108, 109, 111, 113, 115, 119, 121, 122}, '2': {106, 116, 122}, 'root': {106}, 'caus': {106, 114}, 'prevent': {106, 115}, 'recogn': {106}, 'usag': {113, 106}, 'devic': {112, 106, 117}, 'domin': {106}, 'simpl': {106}, 'short': {106}, 'scenario': {106}, 'profil': {106}, 'set': {106, 108}, 'cuttingedg': {106}, 'android': {106}, 'wear': {106}, 'follow': {106, 116}, 'drill': {106}, 'approach': {112, 118, 121, 106, 110}, 'captur': {106}, 'behavior': {106}, 'wide': {106, 122, 119}, 'spectrum': {120, 106}, 'granular': {106, 111}, 'userperceiv': {106}, 'activ': {106}, 'function': {106, 117, 122}, 'call': {106, 119}, 'happen': {106}, 'individu': {120, 106}, 'make': {121, 106, 115, 119}, 'possibl': {106}, 'extens': {106}, 'custom': {106}, 'kernel': {106}, 'facil': {106}, 'suggest': {106, 119}, 'current': {106, 114}, 'far': {106}, 'respons': {106, 122}, 'simpli': {106}, 'updat': {106}, 'display': {106}, 'time': {106, 107, 114, 116, 117, 118, 119, 120, 122}, 'keep': {106}, 'intermitt': {106}, 'busi': {106}, '400': {106}, 'touch': {106}, 'notif': {106}, 'take': {106}, 'second': {121, 106, 115}, 'inherit': {106}, 'much': {106, 115}, 'handheld': {106}, 'exampl': {106, 117}, 'window': {106}, 'manag': {106, 115, 109}, 'often': {112, 106}, 'cpu': {106, 107}, 'ui': {106}, 'task': {113, 106, 107, 108, 109}, 'finish': {106}, 'snap': {106}, 'schedul': {113, 106, 109, 122}, 'interleav': {106}, 'idl': {106}, 'period': {106}, 'unrel': {106}, 'urg': {106}, 'rethink': {106}, 'toward': {106}, 'directli': {112, 106}, 'address': {106, 117, 114, 119}, 'uniqu': {106, 117}, 'align': {107}, 'entir': {107}, 'genom': {107}, 'organ': {107}, 'computeintens': {107}, 'prealign': {107}, 'filter': {107}, 'substanti': {107, 108}, 'reduc': {112, 113, 114, 119, 121, 107}, 'complex': {113, 107, 108}, 'potenti': {112, 107, 108}, 'locat': {107}, 'basecount': {107}, 'success': {107, 119}, 'remov': {121, 107, 119}, '68': {107}, 'histogrambas': {107}, 'heurist': {107}, 'filtpim': {107}, 'base': {115, 116, 117, 118, 119, 107}, 'memrist': {107}, 'processinginmemori': {107}, 'inmemori': {107, 117}, 'cputomemori': {107}, 'transfer': {107, 111}, 'util': {115, 116, 119, 107, 110}, 'intracrossbar': {107}, 'intercrossbar': {107}, 'statefullog': {107}, 'parallel': {115, 107, 117, 118}, 'reduct': {114, 107, 108}, 'togeth': {115, 107}, 'improv': {114, 115, 117, 119, 121, 107, 111}, '100x': {107}, 'implement': {107, 108, 109, 111, 113, 114, 116, 117, 121}, 'recent': {108, 117}, 'year': {108}, 'increasingli': {108, 117, 111}, 'deep': {108}, 'convolut': {113, 108}, 'dcn': {108}, 'boost': {108}, 'imag': {113, 108}, 'recognit': {108}, 'gain': {108}, 'come': {108, 119}, 'cost': {108, 110}, 'increas': {114, 115, 119, 121, 108}, 'storag': {108, 119}, 'resourc': {113, 108, 110}, 'fix': {108}, 'point': {108}, 'ha': {121, 108, 110, 119}, 'allevi': {108}, 'facilit': {108}, 'deploy': {108}, 'hardwar': {116, 121, 108, 109, 110, 111}, 'quantiz': {108}, 'formul': {108}, 'solv': {108, 118}, 'problem': {113, 108, 118, 119}, 'identifi': {114, 108}, 'bitwidth': {108}, 'alloc': {108}, 'across': {116, 115, 108}, 'layer': {116, 108}, 'experi': {108}, 'comparison': {108}, 'equal': {122, 108}, 'bit': {114, 108, 109, 119}, 'width': {108}, 'offer': {121, 108}, '20': {108}, 'size': {116, 113, 114, 108}, 'without': {108}, 'ani': {121, 108}, 'loss': {114, 108}, 'accuraci': {108}, 'cifar10': {108}, 'benchmark': {108}, 'also': {108, 117, 119}, 'demonstr': {108, 117, 118}, 'finetun': {108}, 'enhanc': {108, 117}, 'beyond': {121, 108}, 'origin': {108}, 'float': {108}, 'report': {120, 108}, '678': {108}, 'errorr': {108}, 'abil': {109}, 'either': {109}, 'softwar': {121, 109, 111}, 'import': {112, 121, 115, 109}, 'asset': {109}, 'reconfigur': {109}, 'systemsonchip': {109}, 'featur': {109}, 'develop': {117, 109}, 'combin': {122, 115, 109, 119}, 'suitabl': {109}, 'environ': {109}, 'permit': {109, 118}, 'hardwaresoftwar': {109}, 'relocat': {109}, 'scope': {109}, 'research': {116, 109}, 'scheme': {114, 116, 117, 119, 109}, 'context': {109}, 'switch': {120, 121, 109}, 'issu': {115, 116, 109}, 'prove': {122, 109}, 'allow': {114, 118, 119, 120, 109}, 'u': {109, 118}, 'video': {109}, 'decod': {109}, 'platform': {113, 109}, '23': {109}, 'frames': {109}, '320spl': {109}, 'times240': {109}, 'pixel': {109}, '16': {109}, 'per': {114, 119, 121, 109, 110, 111}, '6': {113, 109}, 'promis': {110, 119}, 'solut': {110}, 'contend': {110}, 'end': {110}, 'dennard': {110}, 'scale': {114, 115, 122, 110, 111}, 'slowdown': {110}, 'moor': {110}, 'law': {110}, 'matur': {110}, 'workload': {113, 115, 110}, 'regular': {118, 110}, 'high': {113, 118, 110, 119}, 'byte': {110}, 'harden': {110}, 'applic': {113, 115, 110, 121}, 'modul': {110}, 'standard': {113, 110}, 'programm': {121, 110}, 'homogen': {110}, 'prefer': {110}, 'previous': {110}, 'heterogen': {110}, 'analyt': {110}, 'queri': {110}, 'systol': {110}, 'array': {117, 110}, 'altern': {113, 110}, 'equival': {110}, 'larg': {114, 115, 110}, 'small': {120, 110}, 'analysi': {110}, 'explain': {110}, 'counterintuit': {110}, 'higher': {110, 119}, 'averag': {115, 110}, 'lower': {114, 110}, 'rel': {110, 111}, 'chip': {111}, 'multiprocessor': {111}, 'continu': {115, 111}, 'mani': {116, 111}, 'throughput': {121, 111}, 'outpac': {111}, 'avail': {111}, 'memori': {111, 113, 114, 115, 116, 117, 118, 119}, 'bandwidth': {113, 111}, 'bottleneck': {113, 121, 111}, 'shift': {111}, 'main': {112, 115, 119, 111}, 'dynam': {120, 122, 111}, 'dgm': {111}, 'unnecessari': {111}, 'save': {111}, 'chang': {114, 119, 111}, 'fine': {111}, 'coarsegrain': {111}, 'access': {113, 115, 116, 111}, 'doe': {121, 114, 111}, 'requir': {113, 119, 118, 111}, 'oper': {114, 115, 116, 119, 111}, 'give': {111}, 'superior': {111}, 'ea': {111}, 'prior': {120, 111}, 'multigranular': {111}, 'maintain': {113, 116, 111}, 'level': {113, 115, 117, 119, 111}, 'glitch': {112}, 'attack': {112}, 'inject': {112}, 'tempor': {112}, 'fault': {112, 119}, 'secur': {112}, 'manipul': {112}, 'hold': {112}, 'sensit': {112, 120}, 'countermeasur': {112}, 'kind': {112}, 'involv': {112, 116, 117}, 'builtin': {112}, 'voltag': {112, 114}, 'regul': {112}, 'anoth': {112, 119}, 'detector': {112}, 'circuit': {112, 117}, 'common': {112, 116}, 'detect': {112}, 'monitor': {112}, 'rail': {112}, 'hard': {112, 119}, 'fast': {112, 118}, 'becom': {112}, 'harder': {112}, 'differenti': {112}, 'nois': {112}, 'silicon': {112}, 'instead': {112}, 'henc': {112, 119}, 'risk': {112}, 'near': {113}, 'camera': {113}, 'everywher': {113}, 'flexibl': {113}, 'sensor': {113}, 'mobil': {113}, 'privaci': {113}, 'reason': {113}, 'local': {113, 115}, 'constraint': {113}, 'dedic': {113, 116}, 'neural': {113}, 'cnn': {113}, 'achiev': {120, 113, 114, 118}, 'target': {113}, 'enough': {113}, 'multipl': {113, 115, 116, 119}, 'vision': {113}, 'limit': {113, 114, 115, 119}, 'amount': {113}, 'extern': {113}, 'hierarchi': {113, 115, 119}, 'onchip': {113}, 'maxim': {113}, 'tile': {113}, 'flow': {113, 118}, 'ensur': {113, 117}, 'evalu': {113, 115, 117, 119}, 'virtex': {113}, 'fpga': {113}, 'board': {113}, 'scratchpad': {113}, '13': {113}, '11': {113}, 'faster': {113, 118}, 'mechan': {114, 115, 116}, 'microprocessor': {114}, 'consumpt': {114, 119}, 'manufacturinginduc': {114}, 'paramet': {114}, 'variat': {120, 114}, 'minimum': {114}, 'vccmin': {114}, 'processor': {114, 116}, 'reliabl': {114, 117}, 'cell': {114, 117, 119}, 'failur': {114}, 'eg': {114, 115}, 'cach': {114, 116}, 'typic': {114}, 'determin': {114}, 'whole': {114}, 'persist': {114}, 'ie': {114, 115}, 'zero': {120, 114}, 'yield': {120, 114}, 'nonpersist': {114}, 'soft': {114, 119}, 'errat': {114}, 'type': {114, 115, 116}, 'suppli': {114}, 'decreas': {114}, 'need': {114}, 'low': {114, 119}, 'novel': {114, 118}, 'adapt': {114}, 'lifetim': {114, 119}, 'multibit': {114}, 'segment': {114}, 'ecc': {114}, 'msecc': {114}, 'like': {114, 115}, 'work': {114}, 'mitig': {114, 115, 117, 119}, 'trade': {114}, 'capac': {114, 115, 116}, 'unlik': {114}, 'reli': {121, 114, 116}, 'isol': {121, 114}, 'defect': {114}, 'therefor': {114, 119}, 'toler': {114}, 'furthermor': {114}, 'capabl': {114, 119}, 'adjust': {114}, 'condit': {114}, 'singlebit': {114}, 'aggress': {114}, '30': {114}, '71': {114}, 'instruct': {114, 116}, '42': {114}, 'footprint': {115}, 'cloud': {115}, 'hpc': {115}, 'fundament': {115}, 'dram': {115, 119}, 'tradit': {115}, 'compos': {115}, 'monolith': {115}, 'greatli': {115}, 'grow': {115}, 'hybrid': {115}, 'pair': {115}, 'nonvolatil': {115}, 'goal': {115, 116}, 'advantag': {115}, 'costeffect': {115}, 'manner': {115}, 'disadvantag': {115}, 'page': {115}, 'place': {115}, 'migrat': {115}, 'within': {115}, 'properti': {115}, 'intellig': {115}, 'placement': {115}, 'decis': {115}, 'affect': {115, 116}, 'performancein': {115}, 'utilitybas': {115}, 'uhmem': {115}, 'variou': {115}, 'systemat': {115}, 'estim': {115}, 'benefit': {121, 115}, 'inform': {115}, 'guid': {115}, 'step': {115, 118}, 'first': {115, 117}, 'singl': {115, 119}, 'would': {115}, 'comprehens': {115}, 'consid': {115}, 'frequenc': {115}, 'row': {115}, 'memorylevel': {115}, 'translat': {115}, 'overal': {115, 119}, 'migrationw': {115}, '14': {115}, '26': {115}, 'best': {115}, 'three': {115}, 'number': {115, 119}, 'dataintens': {115}, 'modern': {116}, 'coher': {120, 116}, 'consist': {116}, 'focu': {116}, 'underli': {116}, 'synchron': {116}, 'resolv': {120, 116}, 'valu': {116, 119}, 'piec': {116}, 'given': {116}, 'instant': {116}, 'discret': {116}, 'way': {116}, 'execut': {116}, 'clock': {116}, 'cycl': {116}, 'correspond': {116}, '4': {116}, 'clear': {116}, 'inconsist': {116}, 'occur': {116, 119}, 'written': {116}, 'notic': {116}, 'read': {116, 119}, 'idiosyncrasi': {116}, 'via': {116}, 'snoopbas': {116}, 'directorybas': {116}, 'top': {116}, 'snoopi': {116}, 'directori': {116}, 'protocol': {121, 116}, 'measur': {120, 116}, 'hit': {116}, 'compulsori': {116}, 'miss': {116}, 'forc': {116}, 'addit': {121, 116}, 'block': {116}, 'rapid': {117}, 'emerg': {117}, 'especi': {117}, 'ferroelectr': {117}, 'fieldeffect': {117}, 'transistor': {117}, 'fefet': {117}, 'densiti': {117, 119}, 'ternari': {117}, 'content': {117}, 'tcam': {117}, 'play': {117}, 'major': {117}, 'role': {117}, 'realiz': {117}, 'braininspir': {117}, 'concept': {117}, 'search': {117}, 'ultradens': {117}, 'ham': {117}, 'distancebas': {117}, 'highlypromis': {117}, 'distanc': {117}, 'inevit': {117}, 'investig': {122, 117}, 'temperatur': {117}, 'fefetbas': {117}, 'well': {120, 117}, 'peripher': {117}, 'depend': {117}, 'help': {117}, 'observ': {117}, 'showcas': {117}, 'discus': {117}, 'elimin': {117}, 'understand': {117}, 'deleteri': {117}, 'domain': {118}, 'decomposit': {118}, 'poisson': {118}, 'equat': {118}, 'irregular': {118}, 'employ': {118}, 'schur': {118}, 'complement': {118}, 'method': {118, 119}, 'multicor': {118}, 'creat': {118}, 'precondition': {118}, 'converg': {118}, 'le': {122, 118, 119}, 'appli': {118}, 'linear': {122, 118}, 'solver': {118}, 'region': {118}, 'subdomain': {118}, 'boundari': {118}, 'fftbase': {118}, '10243': {118}, 'freedom': {118}, 'pressur': {118}, 'project': {118}, 'incompress': {118}, 'liquid': {118}, 'ga': {118}, 'consider': {118, 119}, 'speedup': {118}, 'precondit': {118}, 'conjug': {118}, 'gradient': {118}, 'commonli': {118}, 'includ': {118}, 'multigrid': {118}, 'phase': {119}, 'pcm': {119}, 'candid': {119}, 'due': {119}, 'poor': {119}, 'leakag': {119}, 'costbit': {119}, 'resist': {120, 119}, 'store': {119}, 'rang': {119}, 'mlc': {119}, 'rather': {119}, 'slc': {119}, 'unfortun': {119}, 'expens': {119}, 'longer': {119}, 'readwrit': {119}, 'earlier': {119}, 'wearout': {119}, 'studi': {119}, 'errorpron': {119}, 'write': {119}, 'introduc': {119}, 'den': {119}, 'trilevel': {119}, 'mmetric': {119}, 'metric': {119}, 'deal': {119}, 'extra': {119}, 'line': {119}, 'perman': {119}, 'stuckat': {119}, 'sinc': {119}, 'onli': {121, 119}, 'long': {121, 119}, 'befor': {119}, 'start': {119}, 'articl': {119}, '2bit': {119}, 'relax': {119}, 'intermedi': {119}, 'timeconsum': {119}, 'metadata': {119}, 'section': {119}, 'retriev': {119}, 'exact': {119}, '572': {119}, '561': {119}, 'ipc': {119}, '269': {119}, 'baselin': {119}, 'noteworthi': {119}, 'fpc': {119}, 'compress': {119}, '752': {119}, '67': {119}, '374': {119}, 'singleshot': {120}, 'versu': {120}, 'thermal': {120}, 'assist': {120}, 'spintorqu': {120}, 'magnet': {120}, 'tunnel': {120}, 'junction': {120}, 'dure': {120}, 'view': {120}, 'mode': {120}, 'event': {120}, 'trace': {120}, 'nonequilibrium': {120}, 'excit': {120}, 'precess': {120}, 'amplitud': {120}, 'inplan': {120}, 'hardaxi': {120}, 'field': {120}, 'spatial': {120}, 'forward': {121}, 'plane': {121}, 'breed': {121}, 'might': {121}, 'natur': {121}, 'consensu': {121}, 'focus': {121}, 'paxo': {121}, 'concern': {121}, 'abl': {121}, 'p4base': {121}, 'run': {121}, 'asic': {121}, '25': {121}, 'billion': {121}, 'messag': {121}, 'four': {121}, 'magnitud': {121}, 'widelyus': {121}, 'distribut': {121}, 'center': {121}, 'sheer': {121}, 'readili': {121}, 'lend': {121}, 'formal': {121}, 'verif': {121}, 'full': {121}, 'veri': {121, 122}, 'weak': {121}, 'assumpt': {121}, 'onlin': {122}, 'speed': {122}, 'object': {122}, 'srpt': {122}, 'p': {122}, 'n': {122}, 'shortest': {122}, 'remain': {122}, 'queue': {122}, 'length': {122}, '2competit': {122}, 'class': {122}, 'powerspe': {122}, 'tradeoff': {122}, 'attain': {122}, 'competit': {122}, 'ratio': {122}})\n"
          ]
        }
      ],
      "source": [
        "def indexing_docs(dcm):\n",
        "    a_docs = dcm[0:len(dcm)//2]\n",
        "    b_docs = dcm[len(dcm)//2:]\n",
        "    DocumentBatch(a_docs).sort_based_index_construction()\n",
        "    DocumentBatch(b_docs).sort_based_index_construction()\n",
        "\n",
        "    search_engine = FastSearchEngine()\n",
        "    search_engine.add_batch(DocumentBatch(a_docs))\n",
        "    search_engine.add_batch(DocumentBatch(b_docs))\n",
        "\n",
        "    search_engine.end_of_day_logarithmic_merge()\n",
        "\n",
        "    return search_engine.main_index\n",
        "\n",
        "\n",
        "print(indexing_docs(dcm[100:120])) # چک کردن برای بخش مربوط به ساخت نمایه"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tlu-ZPXAaSCg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def gamma_compression(path):\n",
        "\n",
        "    with open(path, 'wb') as file:\n",
        "        for t, i in indexing_docs(dcm).items():\n",
        "            term = t.encode('utf-8')\n",
        "\n",
        "            sorted_ids = sorted(i)\n",
        "            different = [sorted_ids[0]]\n",
        "            for j in range(1, len(sorted_ids)):\n",
        "                diff = sorted_ids[j] - sorted_ids[j - 1]\n",
        "                different.append(diff)\n",
        "\n",
        "            posting = ''\n",
        "            for d in different:\n",
        "                encoded_d = gamma_encode(d)\n",
        "                for char in encoded_d:\n",
        "                    posting += char\n",
        "\n",
        "            byte_code = int(posting, 2).to_bytes((len(posting) + 7) // 8, byteorder='big')\n",
        "\n",
        "            term_length = len(term).to_bytes(4, 'big')\n",
        "            byte_code_length = len(byte_code).to_bytes(4, 'big')\n",
        "\n",
        "            file.write(term_length)\n",
        "            file.write(term)\n",
        "            file.write(byte_code_length)\n",
        "            file.write(byte_code)\n",
        "\n",
        "\n",
        "def variable_byte_compression(path):\n",
        "\n",
        "    with open(path, 'wb') as file:\n",
        "        for t, i in indexing_docs(dcm).items():\n",
        "            term = t.encode('utf-8')\n",
        "\n",
        "            sorted_i = sorted(i)\n",
        "            diffs = [sorted_i[0]]\n",
        "            for j in range(1, len(sorted_i)):\n",
        "                diff = sorted_i[j] - sorted_i[j - 1]\n",
        "                diffs.append(diff)\n",
        "\n",
        "            encoded_data = []\n",
        "            for d in diffs:\n",
        "                encoded_data += variable_byte_encode(d)\n",
        "\n",
        "            byte_code = b''.join(encoded_data)\n",
        "\n",
        "            term_length = len(term).to_bytes(4, 'big')\n",
        "            byte_code_length = len(byte_code).to_bytes(4, 'big')\n",
        "\n",
        "            file.write(term_length)\n",
        "            file.write(term)\n",
        "            file.write(byte_code_length)\n",
        "            file.write(byte_code)\n",
        "\n",
        "def no_compression(path):\n",
        "    converted_docs = {t: list(p) for t, p in indexing_docs(dcm).items()}\n",
        "\n",
        "    with open(path, 'w', encoding='utf-8') as file:\n",
        "        json.dump(converted_docs, file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAoBHhG6aSCg"
      },
      "outputs": [],
      "source": [
        "def gamma_decode(encoded):\n",
        "    numbers = []\n",
        "    idx = 0\n",
        "\n",
        "    while idx < len(encoded):\n",
        "        next_one = encoded.find('1', idx)\n",
        "        unary_length = next_one - idx\n",
        "\n",
        "        if unary_length < 0:\n",
        "            break\n",
        "\n",
        "        binary_str = '1' + encoded[next_one + 1 : next_one + 1 + unary_length]\n",
        "        idx = next_one + 1 + unary_length\n",
        "\n",
        "        numbers.append(int(binary_str, 2))\n",
        "\n",
        "    return numbers\n",
        "\n",
        "def variable_byte_decode(number):\n",
        "    decoded_numbers = []\n",
        "    current_number = 0\n",
        "    for byte in number:\n",
        "        if byte < 128:\n",
        "            current_number = current_number * 128 + byte\n",
        "        else:\n",
        "            current_number = current_number * 128 + (byte - 128)\n",
        "            decoded_numbers.append(current_number)\n",
        "            current_number = 0\n",
        "    return decoded_numbers\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BI4tHejVaSCg"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from itertools import accumulate\n",
        "\n",
        "def load_gamma(path):\n",
        "\n",
        "    index = {}\n",
        "\n",
        "    with open(path, 'rb') as file:\n",
        "        while True:\n",
        "            term_length = file.read(4)\n",
        "            if not term_length:\n",
        "                break\n",
        "\n",
        "            len_term = int.from_bytes(term_length, 'big')\n",
        "            term = file.read(len_term).decode('utf-8')\n",
        "\n",
        "            data_length = int.from_bytes(file.read(4), 'big')\n",
        "            encoded = file.read(data_length)\n",
        "\n",
        "        bit_str = ''\n",
        "        for byte in encoded:\n",
        "            byte_bin = format(byte, '08b')\n",
        "            for bit in byte_bin:\n",
        "                bit_str += bit\n",
        "\n",
        "        decoded_numbers = gamma_decode(bit_str)\n",
        "\n",
        "        index[term] = list(accumulate(decoded_numbers))\n",
        "\n",
        "    return index\n",
        "\n",
        "\n",
        "def load_variable_byte(path):\n",
        "    index = {}\n",
        "\n",
        "    with open(path, 'rb') as file:\n",
        "        while True:\n",
        "            len_term_bytes = file.read(4)\n",
        "            if not len_term_bytes:\n",
        "                break\n",
        "\n",
        "            len_term = int.from_bytes(len_term_bytes, 'big')\n",
        "            term = file.read(len_term).decode('utf-8')\n",
        "            data_length = int.from_bytes(file.read(4), 'big')\n",
        "            index[term] = list(itertools.accumulate(variable_byte_decode(file.read(data_length))))\n",
        "\n",
        "    return index\n",
        "\n",
        "\n",
        "def load_no_compression(path):\n",
        "    index = {}\n",
        "    with open(path, 'r', encoding='utf-8') as file:\n",
        "        index = json.load(file)\n",
        "    return index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqEh4bYTaSCg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "def save_index(comp_type, path):\n",
        "\n",
        "    types = {\n",
        "        0: no_compression,\n",
        "        1: gamma_compression,\n",
        "        2: variable_byte_compression\n",
        "    }\n",
        "\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        compression_func = types[comp_type]\n",
        "        compression_func(path)\n",
        "    except KeyError:\n",
        "        print(\"Invalid\")\n",
        "\n",
        "def load_index(comp_type, path):\n",
        "\n",
        "    load_functions = {\n",
        "        0: load_no_compression,\n",
        "        1: load_gamma,\n",
        "        2: load_variable_byte\n",
        "    }\n",
        "\n",
        "    load_func = load_functions.get(comp_type)\n",
        "    if load_func:\n",
        "        return load_func(path)\n",
        "    else:\n",
        "        print(\"Invalid\")\n",
        "        return {}\n",
        "\n",
        "def get_size(file_path):\n",
        "    size_of_file = os.path.getsize(file_path)\n",
        "    return ''.join(f\"Size: {'%.3f' % (size_of_file / 1024 ** 2)} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1sfXpqCaSCg",
        "outputId": "d89a51c3-e4a8-4e38-b39d-7896d7d5027e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "save_index(0, \"./indexing.json\")\n",
        "save_index(1, \"./gamma.txt\")\n",
        "save_index(2, \"./variable_byte.txt\")\n",
        "\n",
        "print(\"No-Compression: \" + str(load_index(0, \"./indexing.json\")))\n",
        "print(\"Gamma: \" + str(load_index(1, \"./gamma.txt\")))\n",
        "print(\"VB: \" + str(load_index(2, \"./variable_byte.txt\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIPqIRd3aSCr",
        "outputId": "21d3c152-8789-4688-a19b-2ab826c02b4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No-Compression: Size: 2.960 MB\n",
            "Gamma: Size: 0.935 MB\n",
            "VB: Size: 0.957 MB\n"
          ]
        }
      ],
      "source": [
        "print(\"No-Compression: \" + str(get_size(\"./indexing.json\")))\n",
        "print(\"Gamma: \" + str(get_size(\"./gamma.txt\")))\n",
        "print(\"VB: \" + str(get_size(\"./variable_byte.txt\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9LdZHO9aSCr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "08ac30a6a1fd2e576b33e03f7d61c3a285d7ee0582c2dd23dde6343ef303ebe9"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}